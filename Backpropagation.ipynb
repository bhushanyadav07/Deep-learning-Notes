{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropagation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOSUPou/8esw7RllX0cR6fJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhushanyadav07/Deep-learning-Notes/blob/master/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfImrSw_ou_b",
        "colab_type": "text"
      },
      "source": [
        "ackpropagation exercise\n",
        "Below, you'll implement the code to calculate one backpropagation update step for two sets of weights. I wrote the forward pass - your goal is to code the backward pass.\n",
        "\n",
        "Things to do\n",
        "\n",
        "Calculate the network's output error.\\\n",
        "Calculate the output layer's error term.\\\n",
        "Use backpropagation to calculate the hidden layer's error term.\\\n",
        "Calculate the change in weights (the delta weights) that result from\\\n",
        "propagating the errors back through the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNfX4AhtmuES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "85167a0f-06f6-468b-e067-df04add5bfce"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "x = np.array([0.5, 0.1, -0.2])\n",
        "target = 0.6\n",
        "learnrate = 0.5\n",
        "\n",
        "weights_input_hidden = np.array([[0.5, -0.6],\n",
        "                                 [0.1, -0.2],\n",
        "                                 [0.1, 0.7]])\n",
        "\n",
        "weights_hidden_output = np.array([0.1, -0.3])\n",
        "\n",
        "## Forward pass\n",
        "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
        "hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "output = sigmoid(output_layer_in)\n",
        "\n",
        "## Backwards pass\n",
        "## TODO: Calculate output error\n",
        "error = target - output\n",
        "\n",
        "# TODO: Calculate error term for output layer\n",
        "output_error_term = error * output * (1 - output)\n",
        "\n",
        "# TODO: Calculate error term for hidden layer\n",
        "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
        "                    hidden_layer_output * (1 - hidden_layer_output)\n",
        "\n",
        "# TODO: Calculate change in weights for hidden layer to output layer\n",
        "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
        "\n",
        "# TODO: Calculate change in weights for input layer to hidden layer\n",
        "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
        "\n",
        "print('Change in weights for hidden layer to output layer:')\n",
        "print(delta_w_h_o)\n",
        "print('Change in weights for input layer to hidden layer:')\n",
        "print(delta_w_i_h)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Change in weights for hidden layer to output layer:\n",
            "[0.00804047 0.00555918]\n",
            "Change in weights for input layer to hidden layer:\n",
            "[[ 1.77005547e-04 -5.11178506e-04]\n",
            " [ 3.54011093e-05 -1.02235701e-04]\n",
            " [-7.08022187e-05  2.04471402e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQY5f44go_OL",
        "colab_type": "text"
      },
      "source": [
        "Backpropagation exercise\n",
        "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
        "\n",
        ">Your goals here:\n",
        "\n",
        "Implement the forward pass.\\\n",
        "Implement the backpropagation algorithm.\\\n",
        "Update the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaxXfRIQpR8x",
        "colab_type": "text"
      },
      "source": [
        "###backprop.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E__VMtrSo4jO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from data_prep import features, targets, features_test, targets_test\n",
        "\n",
        "np.random.seed(21)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calculate sigmoid\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_hidden = 2  # number of hidden units\n",
        "epochs = 900\n",
        "learnrate = 0.005\n",
        "\n",
        "n_records, n_features = features.shape\n",
        "last_loss = None\n",
        "# Initialize weights\n",
        "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                        size=(n_features, n_hidden))\n",
        "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
        "                                         size=n_hidden)\n",
        "\n",
        "for e in range(epochs):\n",
        "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
        "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
        "    for x, y in zip(features.values, targets):\n",
        "        ## Forward pass ##\n",
        "        # TODO: Calculate the output\n",
        "        hidden_input = np.dot(x, weights_input_hidden)\n",
        "        hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "        output = sigmoid(np.dot(hidden_output,\n",
        "                                weights_hidden_output))\n",
        "\n",
        "        ## Backward pass ##\n",
        "        # TODO: Calculate the network's prediction error\n",
        "        error = y - output\n",
        "\n",
        "        # TODO: Calculate error term for the output unit\n",
        "        output_error_term = error * output * (1 - output)\n",
        "\n",
        "        ## propagate errors to hidden layer\n",
        "\n",
        "        # TODO: Calculate the hidden layer's contribution to the error\n",
        "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
        "\n",
        "        # TODO: Calculate the error term for the hidden layer\n",
        "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
        "\n",
        "        # TODO: Update the change in weights\n",
        "        del_w_hidden_output += output_error_term * hidden_output\n",
        "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
        "\n",
        "    # TODO: Update weights\n",
        "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
        "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
        "\n",
        "    # Printing out the mean square error on the training set\n",
        "    if e % (epochs / 10) == 0:\n",
        "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
        "        out = sigmoid(np.dot(hidden_output,\n",
        "                             weights_hidden_output))\n",
        "        loss = np.mean((out - targets) ** 2)\n",
        "\n",
        "        if last_loss and last_loss < loss:\n",
        "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
        "        else:\n",
        "            print(\"Train loss: \", loss)\n",
        "        last_loss = loss\n",
        "\n",
        "# Calculate accuracy on test data\n",
        "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
        "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
        "predictions = out > 0.5\n",
        "accuracy = np.mean(predictions == targets_test)\n",
        "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNZvKIIpXgU",
        "colab_type": "text"
      },
      "source": [
        "####Dataprep.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvi7zVuhpQZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "admissions = pd.read_csv('binary.csv')\n",
        "\n",
        "# Make dummy variables for rank\n",
        "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
        "data = data.drop('rank', axis=1)\n",
        "\n",
        "# Standarize features\n",
        "for field in ['gre', 'gpa']:\n",
        "    mean, std = data[field].mean(), data[field].std()\n",
        "    data.loc[:,field] = (data[field]-mean)/std\n",
        "    \n",
        "# Split off random 10% of the data for testing\n",
        "np.random.seed(21)\n",
        "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
        "data, test_data = data.ix[sample], data.drop(sample)\n",
        "\n",
        "# Split into features and targets\n",
        "features, targets = data.drop('admit', axis=1), data['admit']\n",
        "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOqgRsRTpkrw",
        "colab_type": "text"
      },
      "source": [
        "###Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmmi8KJ9pm6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "admit,gre,gpa,rank\n",
        "0,380,3.61,3\n",
        "1,660,3.67,3\n",
        "1,800,4,1\n",
        "1,640,3.19,4\n",
        "0,520,2.93,4\n",
        "1,760,3,2\n",
        "1,560,2.98,1\n",
        "0,400,3.08,2\n",
        "1,540,3.39,3\n",
        "0,700,3.92,2\n",
        "0,800,4,4\n",
        "0,440,3.22,1\n",
        "1,760,4,1\n",
        "0,700,3.08,2\n",
        "1,700,4,1\n",
        "0,480,3.44,3\n",
        "0,780,3.87,4\n",
        "0,360,2.56,3\n",
        "0,800,3.75,2\n",
        "1,540,3.81,1\n",
        "0,500,3.17,3\n",
        "1,660,3.63,2\n",
        "0,600,2.82,4\n",
        "0,680,3.19,4\n",
        "1,760,3.35,2\n",
        "1,800,3.66,1\n",
        "1,620,3.61,1\n",
        "1,520,3.74,4\n",
        "1,780,3.22,2\n",
        "0,520,3.29,1\n",
        "0,540,3.78,4\n",
        "0,760,3.35,3\n",
        "0,600,3.4,3\n",
        "1,800,4,3\n",
        "0,360,3.14,1\n",
        "0,400,3.05,2\n",
        "0,580,3.25,1\n",
        "0,520,2.9,3\n",
        "1,500,3.13,2\n",
        "1,520,2.68,3\n",
        "0,560,2.42,2\n",
        "1,580,3.32,2\n",
        "1,600,3.15,2\n",
        "0,500,3.31,3\n",
        "0,700,2.94,2\n",
        "1,460,3.45,3\n",
        "1,580,3.46,2\n",
        "0,500,2.97,4\n",
        "0,440,2.48,4\n",
        "0,400,3.35,3\n",
        "0,640,3.86,3\n",
        "0,440,3.13,4\n",
        "0,740,3.37,4\n",
        "1,680,3.27,2\n",
        "0,660,3.34,3\n",
        "1,740,4,3\n",
        "0,560,3.19,3\n",
        "0,380,2.94,3\n",
        "0,400,3.65,2\n",
        "0,600,2.82,4\n",
        "1,620,3.18,2\n",
        "0,560,3.32,4\n",
        "0,640,3.67,3\n",
        "1,680,3.85,3\n",
        "0,580,4,3\n",
        "0,600,3.59,2\n",
        "0,740,3.62,4\n",
        "0,620,3.3,1\n",
        "0,580,3.69,1\n",
        "0,800,3.73,1\n",
        "0,640,4,3\n",
        "0,300,2.92,4\n",
        "0,480,3.39,4\n",
        "0,580,4,2\n",
        "0,720,3.45,4\n",
        "0,720,4,3\n",
        "0,560,3.36,3\n",
        "1,800,4,3\n",
        "0,540,3.12,1\n",
        "1,620,4,1\n",
        "0,700,2.9,4\n",
        "0,620,3.07,2\n",
        "0,500,2.71,2\n",
        "0,380,2.91,4\n",
        "1,500,3.6,3\n",
        "0,520,2.98,2\n",
        "0,600,3.32,2\n",
        "0,600,3.48,2\n",
        "0,700,3.28,1\n",
        "1,660,4,2\n",
        "0,700,3.83,2\n",
        "1,720,3.64,1\n",
        "0,800,3.9,2\n",
        "0,580,2.93,2\n",
        "1,660,3.44,2\n",
        "0,660,3.33,2\n",
        "0,640,3.52,4\n",
        "0,480,3.57,2\n",
        "0,700,2.88,2\n",
        "0,400,3.31,3\n",
        "0,340,3.15,3\n",
        "0,580,3.57,3\n",
        "0,380,3.33,4\n",
        "0,540,3.94,3\n",
        "1,660,3.95,2\n",
        "1,740,2.97,2\n",
        "1,700,3.56,1\n",
        "0,480,3.13,2\n",
        "0,400,2.93,3\n",
        "0,480,3.45,2\n",
        "0,680,3.08,4\n",
        "0,420,3.41,4\n",
        "0,360,3,3\n",
        "0,600,3.22,1\n",
        "0,720,3.84,3\n",
        "0,620,3.99,3\n",
        "1,440,3.45,2\n",
        "0,700,3.72,2\n",
        "1,800,3.7,1\n",
        "0,340,2.92,3\n",
        "1,520,3.74,2\n",
        "1,480,2.67,2\n",
        "0,520,2.85,3\n",
        "0,500,2.98,3\n",
        "0,720,3.88,3\n",
        "0,540,3.38,4\n",
        "1,600,3.54,1\n",
        "0,740,3.74,4\n",
        "0,540,3.19,2\n",
        "0,460,3.15,4\n",
        "1,620,3.17,2\n",
        "0,640,2.79,2\n",
        "0,580,3.4,2\n",
        "0,500,3.08,3\n",
        "0,560,2.95,2\n",
        "0,500,3.57,3\n",
        "0,560,3.33,4\n",
        "0,700,4,3\n",
        "0,620,3.4,2\n",
        "1,600,3.58,1\n",
        "0,640,3.93,2\n",
        "1,700,3.52,4\n",
        "0,620,3.94,4\n",
        "0,580,3.4,3\n",
        "0,580,3.4,4\n",
        "0,380,3.43,3\n",
        "0,480,3.4,2\n",
        "0,560,2.71,3\n",
        "1,480,2.91,1\n",
        "0,740,3.31,1\n",
        "1,800,3.74,1\n",
        "0,400,3.38,2\n",
        "1,640,3.94,2\n",
        "0,580,3.46,3\n",
        "0,620,3.69,3\n",
        "1,580,2.86,4\n",
        "0,560,2.52,2\n",
        "1,480,3.58,1\n",
        "0,660,3.49,2\n",
        "0,700,3.82,3\n",
        "0,600,3.13,2\n",
        "0,640,3.5,2\n",
        "1,700,3.56,2\n",
        "0,520,2.73,2\n",
        "0,580,3.3,2\n",
        "0,700,4,1\n",
        "0,440,3.24,4\n",
        "0,720,3.77,3\n",
        "0,500,4,3\n",
        "0,600,3.62,3\n",
        "0,400,3.51,3\n",
        "0,540,2.81,3\n",
        "0,680,3.48,3\n",
        "1,800,3.43,2\n",
        "0,500,3.53,4\n",
        "1,620,3.37,2\n",
        "0,520,2.62,2\n",
        "1,620,3.23,3\n",
        "0,620,3.33,3\n",
        "0,300,3.01,3\n",
        "0,620,3.78,3\n",
        "0,500,3.88,4\n",
        "0,700,4,2\n",
        "1,540,3.84,2\n",
        "0,500,2.79,4\n",
        "0,800,3.6,2\n",
        "0,560,3.61,3\n",
        "0,580,2.88,2\n",
        "0,560,3.07,2\n",
        "0,500,3.35,2\n",
        "1,640,2.94,2\n",
        "0,800,3.54,3\n",
        "0,640,3.76,3\n",
        "0,380,3.59,4\n",
        "1,600,3.47,2\n",
        "0,560,3.59,2\n",
        "0,660,3.07,3\n",
        "1,400,3.23,4\n",
        "0,600,3.63,3\n",
        "0,580,3.77,4\n",
        "0,800,3.31,3\n",
        "1,580,3.2,2\n",
        "1,700,4,1\n",
        "0,420,3.92,4\n",
        "1,600,3.89,1\n",
        "1,780,3.8,3\n",
        "0,740,3.54,1\n",
        "1,640,3.63,1\n",
        "0,540,3.16,3\n",
        "0,580,3.5,2\n",
        "0,740,3.34,4\n",
        "0,580,3.02,2\n",
        "0,460,2.87,2\n",
        "0,640,3.38,3\n",
        "1,600,3.56,2\n",
        "1,660,2.91,3\n",
        "0,340,2.9,1\n",
        "1,460,3.64,1\n",
        "0,460,2.98,1\n",
        "1,560,3.59,2\n",
        "0,540,3.28,3\n",
        "0,680,3.99,3\n",
        "1,480,3.02,1\n",
        "0,800,3.47,3\n",
        "0,800,2.9,2\n",
        "1,720,3.5,3\n",
        "0,620,3.58,2\n",
        "0,540,3.02,4\n",
        "0,480,3.43,2\n",
        "1,720,3.42,2\n",
        "0,580,3.29,4\n",
        "0,600,3.28,3\n",
        "0,380,3.38,2\n",
        "0,420,2.67,3\n",
        "1,800,3.53,1\n",
        "0,620,3.05,2\n",
        "1,660,3.49,2\n",
        "0,480,4,2\n",
        "0,500,2.86,4\n",
        "0,700,3.45,3\n",
        "0,440,2.76,2\n",
        "1,520,3.81,1\n",
        "1,680,2.96,3\n",
        "0,620,3.22,2\n",
        "0,540,3.04,1\n",
        "0,800,3.91,3\n",
        "0,680,3.34,2\n",
        "0,440,3.17,2\n",
        "0,680,3.64,3\n",
        "0,640,3.73,3\n",
        "0,660,3.31,4\n",
        "0,620,3.21,4\n",
        "1,520,4,2\n",
        "1,540,3.55,4\n",
        "1,740,3.52,4\n",
        "0,640,3.35,3\n",
        "1,520,3.3,2\n",
        "1,620,3.95,3\n",
        "0,520,3.51,2\n",
        "0,640,3.81,2\n",
        "0,680,3.11,2\n",
        "0,440,3.15,2\n",
        "1,520,3.19,3\n",
        "1,620,3.95,3\n",
        "1,520,3.9,3\n",
        "0,380,3.34,3\n",
        "0,560,3.24,4\n",
        "1,600,3.64,3\n",
        "1,680,3.46,2\n",
        "0,500,2.81,3\n",
        "1,640,3.95,2\n",
        "0,540,3.33,3\n",
        "1,680,3.67,2\n",
        "0,660,3.32,1\n",
        "0,520,3.12,2\n",
        "1,600,2.98,2\n",
        "0,460,3.77,3\n",
        "1,580,3.58,1\n",
        "1,680,3,4\n",
        "1,660,3.14,2\n",
        "0,660,3.94,2\n",
        "0,360,3.27,3\n",
        "0,660,3.45,4\n",
        "0,520,3.1,4\n",
        "1,440,3.39,2\n",
        "0,600,3.31,4\n",
        "1,800,3.22,1\n",
        "1,660,3.7,4\n",
        "0,800,3.15,4\n",
        "0,420,2.26,4\n",
        "1,620,3.45,2\n",
        "0,800,2.78,2\n",
        "0,680,3.7,2\n",
        "0,800,3.97,1\n",
        "0,480,2.55,1\n",
        "0,520,3.25,3\n",
        "0,560,3.16,1\n",
        "0,460,3.07,2\n",
        "0,540,3.5,2\n",
        "0,720,3.4,3\n",
        "0,640,3.3,2\n",
        "1,660,3.6,3\n",
        "1,400,3.15,2\n",
        "1,680,3.98,2\n",
        "0,220,2.83,3\n",
        "0,580,3.46,4\n",
        "1,540,3.17,1\n",
        "0,580,3.51,2\n",
        "0,540,3.13,2\n",
        "0,440,2.98,3\n",
        "0,560,4,3\n",
        "0,660,3.67,2\n",
        "0,660,3.77,3\n",
        "1,520,3.65,4\n",
        "0,540,3.46,4\n",
        "1,300,2.84,2\n",
        "1,340,3,2\n",
        "1,780,3.63,4\n",
        "1,480,3.71,4\n",
        "0,540,3.28,1\n",
        "0,460,3.14,3\n",
        "0,460,3.58,2\n",
        "0,500,3.01,4\n",
        "0,420,2.69,2\n",
        "0,520,2.7,3\n",
        "0,680,3.9,1\n",
        "0,680,3.31,2\n",
        "1,560,3.48,2\n",
        "0,580,3.34,2\n",
        "0,500,2.93,4\n",
        "0,740,4,3\n",
        "0,660,3.59,3\n",
        "0,420,2.96,1\n",
        "0,560,3.43,3\n",
        "1,460,3.64,3\n",
        "1,620,3.71,1\n",
        "0,520,3.15,3\n",
        "0,620,3.09,4\n",
        "0,540,3.2,1\n",
        "1,660,3.47,3\n",
        "0,500,3.23,4\n",
        "1,560,2.65,3\n",
        "0,500,3.95,4\n",
        "0,580,3.06,2\n",
        "0,520,3.35,3\n",
        "0,500,3.03,3\n",
        "0,600,3.35,2\n",
        "0,580,3.8,2\n",
        "0,400,3.36,2\n",
        "0,620,2.85,2\n",
        "1,780,4,2\n",
        "0,620,3.43,3\n",
        "1,580,3.12,3\n",
        "0,700,3.52,2\n",
        "1,540,3.78,2\n",
        "1,760,2.81,1\n",
        "0,700,3.27,2\n",
        "0,720,3.31,1\n",
        "1,560,3.69,3\n",
        "0,720,3.94,3\n",
        "1,520,4,1\n",
        "1,540,3.49,1\n",
        "0,680,3.14,2\n",
        "0,460,3.44,2\n",
        "1,560,3.36,1\n",
        "0,480,2.78,3\n",
        "0,460,2.93,3\n",
        "0,620,3.63,3\n",
        "0,580,4,1\n",
        "0,800,3.89,2\n",
        "1,540,3.77,2\n",
        "1,680,3.76,3\n",
        "1,680,2.42,1\n",
        "1,620,3.37,1\n",
        "0,560,3.78,2\n",
        "0,560,3.49,4\n",
        "0,620,3.63,2\n",
        "1,800,4,2\n",
        "0,640,3.12,3\n",
        "0,540,2.7,2\n",
        "0,700,3.65,2\n",
        "1,540,3.49,2\n",
        "0,540,3.51,2\n",
        "0,660,4,1\n",
        "1,480,2.62,2\n",
        "0,420,3.02,1\n",
        "1,740,3.86,2\n",
        "0,580,3.36,2\n",
        "0,640,3.17,2\n",
        "0,640,3.51,2\n",
        "1,800,3.05,2\n",
        "1,660,3.88,2\n",
        "1,600,3.38,3\n",
        "1,620,3.75,2\n",
        "1,460,3.99,3\n",
        "0,620,4,2\n",
        "0,560,3.04,3\n",
        "0,460,2.63,2\n",
        "0,700,3.65,2\n",
        "0,600,3.89,3\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}